The human brain is one of the most complex processing systems known to us. Despite the progress that has been made in understanding its behavior and functionality, there are many aspects of it that are still unknown. One major breakthrough was the discovery of the neural network and, later on, the introduction of Artificial Intelligence in the late 50s which was heavily influenced by real neural networks. On the other hand, computer systems have been evolving in parallel with AI, and it has been possible to obtain major improvements on computations that the brain itself is not capable of. As an example, computers can perform billions of mathematical computations per second, beating up humans by a wide margin, but mainstream architectures of computer systems do not have the ability to achieve the capacity of the brain, specially in areas like patter recognition, learning capabilities and self adaptation. 

Brain-inspired computing is a novel information processing technology, which learns from the basic rules of how human brain processes information and makes improvement on the existing computing technology based on these rules. Brain-inspired computing covers a wide variety of topics such as software algorithms and hardware systems. In the case of software algorithms, the most pervasive application is the Artificial Neural Network (ANN). The ANN consist of neurons, connections and hierarchical architectures inspired by neuron systems found in human brains. Although early ANNs were relatively simple and had limited functionality researchers have deepened the neural networks by increasing the number of layers and the complexity of their connection, resulting in the concept of "deep learning" developing rapidly. This has enabled a major breakthrough on massive cognition tasks such as speech recognition and image recognition leading to great advances like self driving cars. However, as the neural networks becoming deeper, the number of neurons and connections drastically increases, which inevitably results in the more urgent needs of computing power. For example, the GoogLeNet, proposed by Google Brain, consists of 22 layers and more 5 million parameters. 

It is then fair to ask what has lead great advances in the areas of AI (and related) in the recent years, despite of a decades long history of these fields? The answer can be easily found when the history of parallel computer systems and brain-inspired computation are placed side by side. Parallel computing was introduced around the mid 60's and great advances were made in parallel computing in the 70's and 80's. Still, the success of sequential computation that was driven by an aggressive technology scaling, as well as a well defined model of computation, lead parallel computing through a dark period in the 90's that was well described by Ken Kennedy in his article "is parallel computing dead?" \cite{kenedy96}. At the same time, interest in AI decreased due to the lack of viability in terms of computation and emulation. However at the beginning of the 2000's single core architectures starting to experience major limitations in power consumption and memory bandwidth, resulting in a degraded scaling of systems. These brought up interest on parallel computing again, and in a second spring for parallel systems and unconventional computer architectures. At the same time the internet revolution and other aspects brought back interest in AI and brain inspired computation. It is now clear that being able to emulate the behavior of these huge neural network requires multiple CPUs, or even GPUs to perform the parallel computing. The wide availability of both, is driving the world to a second spring of AI and parallel computing that could have major implications at all levels of our society. 

However, it has been challenging to build a brain-inspired computing system using parallel computing. Among many more, there are three challenges that are critical: The first challenge lies in determining the granularity of the parallelism given the system architecture and size of the application. The second challenge is how we can build a brain-inspired computing system that scales up with the number of cores by using fine-grain parallel schemes. The third challenge is how to measure the effectiveness of such system and how to justify and locate the source of changes in effectiveness and performance of brain-inspired applications. 

For the first challenge, a decision should be made between coarse-grained and fine-grained parallelism. Todayâ€™s widely adopted parallel programming models, including MPI, OpenMP and other derived models, are all extended from the Bulk Synchronous Parallel (BSP) model. Nevertheless, BSP-like models have shown serious bottlenecks in part due to the cost of synchronization which scales up with the size of the system. This is worsen by the strict sequential memory models enforced by current computer architectures, and the variability in core's behavior and execution time, among other aspects. Resulting in poor scalability with the number of cores, leading to the increase in the CPU idle time, and reducing the desired performance gain expected in parallel systems. Additionally to these, and possibly the most negative drawback lies in that it model does not solve the bottleneck of memory access under the traditional Von Neumann architecture, which severely restrict the potential of brain-inspired computing systems and software. Therefore, it is necessary to move away from coarse grain parallelism in order to address the above bottlenecks. We believe that find-grained parallelism could increase the ability of parallel computers to deliver performance scaling, and providing high-performance parallel acceleration for brain-inspired computing.
Then comes the second challenge, how can we build a brain-inspired computing system based on fine-grained parallelism. Fortunately, the dataflow model of computation proposed in 1974 by Jack Dennis \cite{JackB74} provides a parallel execution architecture that supports fine-grained parallelism.

In the dataflow model of computation, the execution of instructions is not controlled by the program counter, but rather it depends on whether the required input data is available or not. This is, when all the input data of an instruction is available, the instruction is then ready to be executed. When the instruction has finished execution, the output becomes available to the following instructions that depend on such result. This asynchronous execution manner of the dataflow model is able to provide the fine-grained parallelism as low as instruction level. For this reason, dataflow models not only provide fine-grain synchronization, but also the data dependency pattern presented between instructions is similar to the signal transmission mechanism between neurons in the artificial neural networks. This empowers dataflow based execution model and architectures with the capability to emulate the existing artificial neural networks, as well as the potential to introduce more sophisticated interaction mechanisms between neurons.

The Codelet model, proposed by Guang R. Gao in 2011, is a fine-grained parallel Program Execution Model (PXM) that derives from the idea of asynchronous execution of the dataflow model, while borrowing ideas and concepts of the Von Neumann model. Compared to the dataflow model, the extension of Codelet model lies in: First, Codelets are not single instructions, but a collection of instructions that are functional and side effect free. These instructions form a task of the Codelet Graph, and could be executed in Von Neumann architectures. In addition Codelets are not only data-driven, but also event-driven. In Codelet model, the execution of a program depends not only on the availability of input data but also on the machine resources and events. Second, it uses threaded Procedures, which are containers for the execution environment of Codelets and that asynchronous function calls that use continuation signals to return values to codelets in the Caller's environment. Codelets still rely on fine-grained parallelism that enables more flexible resource allocation, but they use argument fetching data-flow, which assembles an architecture closer to Von Neumann architectures that are widely available and studied. Therefore, it is of great significance if we could study the codelet model when applied to brain-inspired computing.

Finally, for the third challenge, we need to apply our system to benchmarks that are classical representations of artificial neural networks to indicate the practical advantage of our system. Also, we need to provide evidence about how computation are accelerated during execution, in order to support the arguments about fine-grained parallelism. 

Aiming at addressing the above-mentioned challenges, this paper mainly have three contributions as follow:
\begin{itemize}
    \item We use the fine-grained multi-thread parallelism of Codelet model as an initial attempt to solve the technical bottleneck faced by the existing coarse-grained parallelism and provide higher performance parallel acceleration for the simulation of artificial neural network.
    \item To our knowledge, this is the first work that applies the Codelet model to brain-inspired computing, especially to the simulation of neural networks, providing a novel path for high-performance brain-inspired simulation platforms.
    \item We evaluate our system based on the most pervasively used neural network, LeNet5, to demonstrate the feasibility and superiority of the codelet model. We also conduct study on the source of effectiveness of fine-grained parallelism by profiling the execution of our system. 

\end{itemize}


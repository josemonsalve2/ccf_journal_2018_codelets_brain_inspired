Currently, the most commonly used neural network simulation framework is Tensorflow developed by the Google brain. Tensorflow executes neural networks in the form of dataflow graphs (DFG). The nodes in the DFG represent math operations, and the edges in the DFG represent the interactions between operations. Tensorflow can flexibly deploy the computation task of a neural network on arbitrary servers with CPUs and GPUs, or on mobile devices. Specifically, the DFG is partitioned into several parts and the computation of each part are allocated to different devices for execution.
Due to the growing model size of the neural networks, plenty of research focus on the parallel acceleration of training and inference of the neural networks. Yunji Chen discovered that the memory footprint of convolutional and deep neural networks (CNNs and DNNs), while large, is not beyond the capability of the on chip storage of a multi-chip system. They developed DaDianNao, a custom multi-chip machine-learning architecture that enables high-degree parallelism of the execution of neural networks at a reasonable area cost. Song Yao et al. started from the algorithm aspect of neural networks. They leveraged pruning to reduce the amount of connections, as well as quantification to reduce the memory usage of each parameter. At the same time, Yao adopted software and hardware co-optimization strategy, which can provide faster acceleration of the execution of neural networks from the hardware aspects. On these basis, Yao et al. proposed Efficient Speech Engine (ESE) and Efficient Inference Engine (EIE).
Generally, the optimization of parallel acceleration of neural networks can proceed from two aspects - structure and algorithm. The former achieves parallel computing with less overhead by changing computation mode and memory access mode. The latter increase the execution speed and storage efficiency of the neural networks by reducing the amount of connections and uneven quantification of parameters. In the future, with the rapid evolution of neural networks, various new structures and algorithms aimed at efficiently parallel acceleration are still evolving rapidly.
With respect to the works about dataflow model. Intel recently released an exascale dataflow engine, i.e. configurable spatial accelerator, which implements the dataflow program execution model on the hardware level. The engine is primarily consist of several computation processing elements, interconnected networks and in-fabric storage elements. This structure of CSA is physical implementation of the underlying abstract hardware of coldelet model, which enables CSA to support the similar task suit for codelet model such as neural networks execution.  

